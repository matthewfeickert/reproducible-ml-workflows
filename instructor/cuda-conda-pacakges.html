<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Reproducible Machine Learning Workflows for Scientists: CUDA conda packages</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="../assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="../assets/styles.css"><script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../favicons/incubator/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicons/incubator/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="../favicons/incubator/favicon-16x16.png"><link rel="manifest" href="../favicons/incubator/site.webmanifest"><link rel="mask-icon" href="../favicons/incubator/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Lesson Description" src="../assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='../cuda-conda-pacakges.html';">Learner View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Lesson Description" src="../assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Reproducible Machine Learning Workflows for Scientists
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Reproducible Machine Learning Workflows for Scientists
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><hr><li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="../instructor/aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Reproducible Machine Learning Workflows for Scientists
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 41%" class="percentage">
    41%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 41%" aria-valuenow="41" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../cuda-conda-pacakges.html">Learner View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="introduction.html">1. Reproducible Research</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="pixi-intro.html">2. Introduction to Pixi</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="backwards-compatibility.html">3. Backwards compatibility with conda</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="conda-pacakges.html">4. Conda packages</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        5. CUDA conda packages
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#cuda">CUDA</a></li>
<li><a href="#cuda-on-conda-forge">CUDA on conda-forge</a></li>
<li><a href="#cuda-use-with-pixi">CUDA use with Pixi</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="pixi-deployment.html">6. Deploying Pixi environments with Linux containers</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="htc-systems.html">7. Using Pixi environments on HTC Systems</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr><li><a class="dropdown-item" href="reference.html">Reference</a></li>
                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources"><a href="../instructor/aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/conda-pacakges.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/pixi-deployment.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/conda-pacakges.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Conda packages
        </a>
        <a class="chapter-link float-end" href="../instructor/pixi-deployment.html" rel="next">
          Next: Deploying Pixi...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>CUDA conda packages</h1>
        <p>Last updated on 2025-10-28 |

        <a href="https://github.com/carpentries-incubator/reproducible-ml-workflows/edit/main/episodes/cuda-conda-pacakges.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 45 minutes</p>

        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>What is CUDA?</li>
<li>How can I use CUDA enabled conda packages?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>Understand how CUDA can be used with conda packages</li>
<li>Create a hardware accelerated environment</li>
</ul></div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="cuda">CUDA<a class="anchor" aria-label="anchor" href="#cuda"></a></h2>
<hr class="half-width"><p>CUDA (Compute Unified Device Architecture) <a href="https://developer.nvidia.com/cuda-zone" class="external-link">is a parallel computing
platform and programming model developed by NVIDIA for general computing
on graphical processing units (GPUs)</a>. The CUDA ecosystem provides
software developer software development kits (SDK) with APIs to CUDA
that allow for software developers to write hardware accelerated
programs with CUDA in various languages for NVIDIA GPUs. CUDA supports a
number of languages including C, C++, Fortran, Python, and Julia. While
there are other types of hardware acceleration development platforms, as
of 2025 CUDA is the most abundant platform for scientific computing that
uses GPUs and effectively the default choice for major machine learning
libraries and applications.</p>
<p>CUDA is closed source and proprietary to NVIDIA, which means that
NVIDIA has historically limited the download access of the CUDA toolkits
and drivers to registered NVIDIA developers (while keeping the software
free (monetarily) to use). CUDA then required a <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/" class="external-link">multi-step
installation process</a> with manual steps and decisions based on the
target platform and particular CUDA version. This meant that when CUDA
enabled environments were setup on a particular machine they were
powerful and optimized, but brittle to change and could easily be broken
if system wide updates (like for security fixes) occurred. CUDA software
environments were bespoke and not many scientists understood how to
construct and curate them.</p>
</section><section><h2 class="section-heading" id="cuda-on-conda-forge">CUDA on conda-forge<a class="anchor" aria-label="anchor" href="#cuda-on-conda-forge"></a></h2>
<hr class="half-width"><p>In <a href="https://github.com/conda-forge/conda-forge.github.io/issues/687" class="external-link">late
2018</a> to better support the scientific developer community, NVIDIA
started to release components of the CUDA toolkits on the <a href="https://anaconda.org/nvidia" class="external-link"><code>nvidia</code> conda
channel</a>. This provided the first access to start to create conda
environments where the versions of different CUDA tools could be
directly specified and downloaded. However, all of this work was being
done internally in NVIDIA and as it was on a separate channel it was
less visible and it still required additional knowledge to work with. In
<a href="https://youtu.be/WgKwlGgVzYE?si=hfyAo6qLma8hnJ-N" class="external-link">2023</a>,
NVIDIA’s open source team began to move the release of CUDA conda
packages from the <code>nvidia</code> channel to conda-forge, making it
easier to discover and allowing for community support. With significant
advancements in system driver specification support, CUDA 12 became the
first version of CUDA to be released as conda packages through
conda-forge and included all CUDA libraries from the <a href="https://github.com/conda-forge/cuda-nvcc-feedstock" class="external-link">CUDA compiler
<code>nvcc</code></a> to the <a href="https://github.com/conda-forge/cuda-libraries-dev-feedstock" class="external-link">CUDA
development libraries</a>. They also released <a href="https://github.com/conda-forge/cuda-feedstock/" class="external-link">CUDA
metapackages</a> that allowed users to easily describe the version of
CUDA they required (e.g. <code>cuda-version=12.5</code>) and the CUDA
conda packages they wanted (e.g. <code>cuda</code>). This significantly
improved the ability for researchers to easily create CUDA accelerated
computing environments.</p>
<p>This is all possible via use of the <code>__cuda</code> <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-virtual.html" class="external-link">virtual
conda package</a>, which is determined automatically by conda package
managers from the hardware information associated with the machine the
package manager is installed on.</p>
<p>With Pixi, a user can get this information with <a href="https://pixi.sh/latest/advanced/explain_info_command/" class="external-link"><code>pixi info</code></a>,
which could have output that looks something like</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="ex">pixi</span> info</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>System
------------
       Pixi version: 0.50.2
           Platform: linux-64
   Virtual packages: __unix=0=0
                   : __linux=6.8.0=0
                   : __glibc=2.35=0
                   : __cuda=12.9=0
                   : __archspec=1=skylake
          Cache dir: /home/&lt;username&gt;/.cache/rattler/cache
       Auth storage: /home/&lt;username&gt;/.rattler/credentials.json
   Config locations: No config files found

Global
------------
            Bin dir: /home/&lt;username&gt;/.pixi/bin
    Environment dir: /home/&lt;username&gt;/.pixi/envs
       Manifest dir: /home/&lt;username&gt;/.pixi/manifests/pixi-global.toml
</code></pre>
</div>
</section><section><h2 class="section-heading" id="cuda-use-with-pixi">CUDA use with Pixi<a class="anchor" aria-label="anchor" href="#cuda-use-with-pixi"></a></h2>
<hr class="half-width"><p>To be able to effectively use CUDA conda packages with Pixi, we make
use of Pixi’s <a href="https://pixi.sh/latest/workspace/system_requirements/" class="external-link">system
requirement workspace table</a>, which specifies the
<strong>minimum</strong> system specifications needed to install and run
a Pixi workspace’s environments.</p>
<p>To do this for CUDA, we just add the minimum supported CUDA version
(based on the host machine’s NVIDIA driver API) we want to support to
the table.</p>
<p>Example:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">TOML<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode toml" tabindex="0"><code class="sourceCode toml"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="kw">[system-requirements]</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="dt">cuda</span> <span class="op">=</span> <span class="st">"12"</span>  <span class="co"># Replace "12" with the specific CUDA version you intend to use</span></span></code></pre>
</div>
<p>This ensures that packages depending on
<code>__cuda &gt;= {version}</code> are resolved correctly.</p>
<p>To demonstrate this a bit more explicitly, we can create a minimal
project</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="ex">pixi</span> init ~/pixi-cuda-lesson/cuda-example</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="bu">cd</span> ~/pixi-cuda-lesson/cuda-example</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>✔ Created /home/&lt;username&gt;/pixi-cuda-lesson/cuda-example/pixi.toml</code></pre>
</div>
<p>where we specify a <code>cuda</code> system requirement</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="ex">pixi</span> workspace system-requirements add cuda 12</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">TOML<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode toml" tabindex="0"><code class="sourceCode toml"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">[workspace]</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="dt">channels</span> <span class="op">=</span> <span class="op">[</span><span class="st">"conda-forge"</span><span class="op">]</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="dt">name</span> <span class="op">=</span> <span class="st">"cuda-example"</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="dt">platforms</span> <span class="op">=</span> <span class="op">[</span><span class="st">"linux-64"</span><span class="op">]</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="dt">version</span> <span class="op">=</span> <span class="st">"0.1.0"</span></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="kw">[tasks]</span></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a><span class="kw">[dependencies]</span></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a><span class="kw">[system-requirements]</span></span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a><span class="dt">cuda</span> <span class="op">=</span> <span class="st">"12"</span></span></code></pre>
</div>
<div id="system-requirements-table-cant-be-target-specific" class="callout caution">
<div class="callout-square">
<i class="callout-icon" data-feather="alert-triangle"></i>
</div>
<span class="callout-header">Caution</span>
<div id="system-requirements-table-cant-be-target-specific" class="callout-inner">
<h3 class="callout-title">
<code>system-requirements</code> table can’t
be target specific</h3>
<div class="callout-content">
<p>As of <a href="https://github.com/prefix-dev/pixi/releases/tag/v0.50.2" class="external-link">Pixi
<code>v0.50.2</code></a>, <a href="https://github.com/prefix-dev/pixi/issues/2714" class="external-link">the
<code>system-requirements</code> table can’t be target specific</a>. To
work around this, if you’re on a platform that doesn’t support the
<code>system-requirements</code> it will ignore them without erroring
unless they are required for the platform specific packages or actions
you have. So, for example, you can have <code>osx-arm64</code> as a
platform and a <code>system-requirements</code> of
<code>cuda = "12"</code> defined</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">TOML<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode toml" tabindex="0"><code class="sourceCode toml"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="kw">[workspace]</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="dt">...</span></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="dt">platforms</span> <span class="op">=</span> <span class="op">[</span><span class="st">"linux-64"</span><span class="op">]</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a><span class="dt">...</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a><span class="kw">[system-requirements]</span></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a><span class="dt">cuda</span> <span class="op">=</span> <span class="st">"12"</span></span></code></pre>
</div>
<p>Pixi will ignore that requirement unless you try to use CUDA packages
in <code>osx-arm64</code> environments.</p>
</div>
</div>
</div>
<p>and then install the <a href="https://github.com/conda-forge/cuda-version-feedstock/blob/main/recipe/README.md" class="external-link"><code>cuda-version</code>
metapacakge</a></p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="ex">pixi</span> add <span class="st">"cuda-version 12.9.*"</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>✔ Added cuda-version 12.9.*</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">TOML<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode toml" tabindex="0"><code class="sourceCode toml"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="kw">[workspace]</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="dt">channels</span> <span class="op">=</span> <span class="op">[</span><span class="st">"conda-forge"</span><span class="op">]</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="dt">name</span> <span class="op">=</span> <span class="st">"cuda-example"</span></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="dt">platforms</span> <span class="op">=</span> <span class="op">[</span><span class="st">"linux-64"</span><span class="op">]</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a><span class="dt">version</span> <span class="op">=</span> <span class="st">"0.1.0"</span></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a><span class="kw">[tasks]</span></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a><span class="kw">[dependencies]</span></span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a><span class="dt">cuda-version</span> <span class="op">=</span> <span class="st">"12.9.*"</span></span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a><span class="kw">[system-requirements]</span></span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a><span class="dt">cuda</span> <span class="op">=</span> <span class="st">"12"</span></span></code></pre>
</div>
<p>If we look at the metadata installed by the <code>cuda-version</code>
package (the only thing it does)</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="fu">cat</span> .pixi/envs/default/conda-meta/cuda-version-<span class="pp">*</span>.json</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">JSON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode json" tabindex="0"><code class="sourceCode json"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>  <span class="dt">"build"</span><span class="fu">:</span> <span class="st">"h4f385c5_3"</span><span class="fu">,</span></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>  <span class="dt">"build_number"</span><span class="fu">:</span> <span class="dv">3</span><span class="fu">,</span></span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>  <span class="dt">"constrains"</span><span class="fu">:</span> <span class="ot">[</span></span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>    <span class="st">"cudatoolkit 12.9|12.9.*"</span><span class="ot">,</span></span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>    <span class="st">"__cuda &gt;=12"</span></span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>  <span class="ot">]</span><span class="fu">,</span></span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>  <span class="dt">"depends"</span><span class="fu">:</span> <span class="ot">[]</span><span class="fu">,</span></span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a>  <span class="dt">"license"</span><span class="fu">:</span> <span class="st">"LicenseRef-NVIDIA-End-User-License-Agreement"</span><span class="fu">,</span></span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a>  <span class="dt">"md5"</span><span class="fu">:</span> <span class="st">"b6d5d7f1c171cbd228ea06b556cfa859"</span><span class="fu">,</span></span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a>  <span class="dt">"name"</span><span class="fu">:</span> <span class="st">"cuda-version"</span><span class="fu">,</span></span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a>  <span class="dt">"noarch"</span><span class="fu">:</span> <span class="st">"generic"</span><span class="fu">,</span></span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a>  <span class="dt">"sha256"</span><span class="fu">:</span> <span class="st">"5f5f428031933f117ff9f7fcc650e6ea1b3fef5936cf84aa24af79167513b656"</span><span class="fu">,</span></span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a>  <span class="dt">"size"</span><span class="fu">:</span> <span class="dv">21578</span><span class="fu">,</span></span>
<span id="cb13-15"><a href="#cb13-15" tabindex="-1"></a>  <span class="dt">"subdir"</span><span class="fu">:</span> <span class="st">"noarch"</span><span class="fu">,</span></span>
<span id="cb13-16"><a href="#cb13-16" tabindex="-1"></a>  <span class="dt">"timestamp"</span><span class="fu">:</span> <span class="dv">1746134436166</span><span class="fu">,</span></span>
<span id="cb13-17"><a href="#cb13-17" tabindex="-1"></a>  <span class="dt">"version"</span><span class="fu">:</span> <span class="st">"12.9"</span><span class="fu">,</span></span>
<span id="cb13-18"><a href="#cb13-18" tabindex="-1"></a>  <span class="dt">"fn"</span><span class="fu">:</span> <span class="st">"cuda-version-12.9-h4f385c5_3.conda"</span><span class="fu">,</span></span>
<span id="cb13-19"><a href="#cb13-19" tabindex="-1"></a>  <span class="dt">"url"</span><span class="fu">:</span> <span class="st">"https://conda.anaconda.org/conda-forge/noarch/cuda-version-12.9-h4f385c5_3.conda"</span><span class="fu">,</span></span>
<span id="cb13-20"><a href="#cb13-20" tabindex="-1"></a>  <span class="dt">"channel"</span><span class="fu">:</span> <span class="st">"https://conda.anaconda.org/conda-forge/"</span><span class="fu">,</span></span>
<span id="cb13-21"><a href="#cb13-21" tabindex="-1"></a>  <span class="dt">"extracted_package_dir"</span><span class="fu">:</span> <span class="st">"/home/&lt;username&gt;/.cache/rattler/cache/pkgs/cuda-version-12.9-h4f385c5_3"</span><span class="fu">,</span></span>
<span id="cb13-22"><a href="#cb13-22" tabindex="-1"></a>  <span class="dt">"files"</span><span class="fu">:</span> <span class="ot">[]</span><span class="fu">,</span></span>
<span id="cb13-23"><a href="#cb13-23" tabindex="-1"></a>  <span class="dt">"paths_data"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb13-24"><a href="#cb13-24" tabindex="-1"></a>    <span class="dt">"paths_version"</span><span class="fu">:</span> <span class="dv">1</span><span class="fu">,</span></span>
<span id="cb13-25"><a href="#cb13-25" tabindex="-1"></a>    <span class="dt">"paths"</span><span class="fu">:</span> <span class="ot">[]</span></span>
<span id="cb13-26"><a href="#cb13-26" tabindex="-1"></a>  <span class="fu">},</span></span>
<span id="cb13-27"><a href="#cb13-27" tabindex="-1"></a>  <span class="dt">"link"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb13-28"><a href="#cb13-28" tabindex="-1"></a>    <span class="dt">"source"</span><span class="fu">:</span> <span class="st">"/home/&lt;username&gt;/.cache/rattler/cache/pkgs/cuda-version-12.9-h4f385c5_3"</span><span class="fu">,</span></span>
<span id="cb13-29"><a href="#cb13-29" tabindex="-1"></a>    <span class="dt">"type"</span><span class="fu">:</span> <span class="dv">1</span></span>
<span id="cb13-30"><a href="#cb13-30" tabindex="-1"></a>  <span class="fu">}</span></span>
<span id="cb13-31"><a href="#cb13-31" tabindex="-1"></a><span class="fu">}</span></span></code></pre>
</div>
<p>we see that it now enforces constraints on the versions of
<code>cudatoolkit</code> that can be installed as well as the required
<code>__cuda</code> virtual package provided by the system</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">JSON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode json" tabindex="0"><code class="sourceCode json"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>  <span class="er">...</span></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>  <span class="dt">"constrains"</span><span class="fu">:</span> <span class="ot">[</span></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>    <span class="st">"cudatoolkit 12.9|12.9.*"</span><span class="ot">,</span></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>    <span class="st">"__cuda &gt;=12"</span></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>  <span class="ot">]</span><span class="fu">,</span></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>  <span class="er">...</span></span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a><span class="fu">}</span></span></code></pre>
</div>
<div id="use-the-feature-table-to-solve-environment-that-your-platform-doesnt-support" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="use-the-feature-table-to-solve-environment-that-your-platform-doesnt-support" class="callout-inner">
<h3 class="callout-title">Use the <code>feature</code> table to solve
environment that your platform doesn’t support</h3>
<div class="callout-content">
<p>CUDA is supported only by NVIDIA GPUs, which means that macOS
operating system platforms (<code>osx-64</code>, <code>osx-arm64</code>)
can’t support it. Similarly, if you machine doesn’t have an NVIDIA GPU,
then the <code>__cuda</code> virtual package won’t exist and installs of
CUDA packages will fail. However, there’s many situations in which you
want to <strong>solve and environment for a platform that you don’t
have</strong> and we can do this for CUDA as well.</p>
<p>If we make the Pixi workspace multiplatform</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="ex">pixi</span> workspace platform add linux-64 osx-arm64 win-64</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>✔ Added linux-64
✔ Added osx-arm64
✔ Added win-64</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">TOML<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode toml" tabindex="0"><code class="sourceCode toml"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="kw">[workspace]</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a><span class="dt">channels</span> <span class="op">=</span> <span class="op">[</span><span class="st">"conda-forge"</span><span class="op">]</span></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a><span class="dt">name</span> <span class="op">=</span> <span class="st">"cuda-example"</span></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a><span class="dt">platforms</span> <span class="op">=</span> <span class="op">[</span><span class="st">"linux-64"</span><span class="op">,</span> <span class="st">"osx-arm64"</span><span class="op">,</span> <span class="st">"win-64"</span><span class="op">]</span></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a><span class="dt">version</span> <span class="op">=</span> <span class="st">"0.1.0"</span></span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a><span class="kw">[tasks]</span></span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a><span class="kw">[dependencies]</span></span></code></pre>
</div>
<p>We can then use Pixi’s <a href="https://pixi.sh/latest/reference/pixi_manifest/#the-target-table" class="external-link">platform
specific <code>target</code> tables</a> to add dependencies for an
environment to only a specific platform. So, if we know that a
dependency only exists for platform <platform> then we can have Pixi add
it for only that platform with</platform></p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="ex">pixi</span> add <span class="at">--platform</span> <span class="op">&lt;</span>platform<span class="op">&gt;</span> <span class="op">&lt;</span>dependency<span class="op">&gt;</span></span></code></pre>
</div>
</div>
</div>
</div>
<p>This now means that if we ask for any CUDA enbabled packages, we will
get ones that are built to support <code>cudatoolkit</code>
<code>v12.9.*</code></p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="ex">pixi</span> add <span class="at">--platform</span> linux-64 cuda</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>✔ Added cuda &gt;=12.9.1,&lt;13
Added these only for platform(s): linux-64</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="ex">pixi</span> list <span class="at">--platform</span> linux-64 cuda</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Package                      Version  Build       Size       Kind   Source
cuda                         12.9.1   ha804496_0  26.7 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-cccl_linux-64           12.9.27  ha770c72_0  1.1 MiB    conda  https://conda.anaconda.org/conda-forge/
cuda-command-line-tools      12.9.1   ha770c72_0  20 KiB     conda  https://conda.anaconda.org/conda-forge/
cuda-compiler                12.9.1   hbad6d8a_0  20.2 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-crt-dev_linux-64        12.9.86  ha770c72_2  92 KiB     conda  https://conda.anaconda.org/conda-forge/
cuda-crt-tools               12.9.86  ha770c72_2  28.5 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-cudart                  12.9.79  h5888daf_0  22.7 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-cudart-dev              12.9.79  h5888daf_0  23.1 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-cudart-dev_linux-64     12.9.79  h3f2d84a_0  380 KiB    conda  https://conda.anaconda.org/conda-forge/
cuda-cudart-static           12.9.79  h5888daf_0  22.7 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-cudart-static_linux-64  12.9.79  h3f2d84a_0  1.1 MiB    conda  https://conda.anaconda.org/conda-forge/
cuda-cudart_linux-64         12.9.79  h3f2d84a_0  192.6 KiB  conda  https://conda.anaconda.org/conda-forge/
cuda-cuobjdump               12.9.82  hbd13f7d_0  237.5 KiB  conda  https://conda.anaconda.org/conda-forge/
cuda-cupti                   12.9.79  h9ab20c4_0  1.8 MiB    conda  https://conda.anaconda.org/conda-forge/
cuda-cupti-dev               12.9.79  h9ab20c4_0  4.4 MiB    conda  https://conda.anaconda.org/conda-forge/
cuda-cuxxfilt                12.9.82  hbd13f7d_0  211.4 KiB  conda  https://conda.anaconda.org/conda-forge/
cuda-driver-dev              12.9.79  h5888daf_0  22.5 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-driver-dev_linux-64     12.9.79  h3f2d84a_0  36.8 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-gdb                     12.9.79  ha677faa_0  378.2 KiB  conda  https://conda.anaconda.org/conda-forge/
cuda-libraries               12.9.1   ha770c72_0  20 KiB     conda  https://conda.anaconda.org/conda-forge/
cuda-libraries-dev           12.9.1   ha770c72_0  20.1 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-nsight                  12.9.79  h7938cbb_0  113.2 MiB  conda  https://conda.anaconda.org/conda-forge/
cuda-nvcc                    12.9.86  hcdd1206_1  24.3 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-nvcc-dev_linux-64       12.9.86  he91c749_2  27.5 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-nvcc-impl               12.9.86  h85509e4_2  26.6 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-nvcc-tools              12.9.86  he02047a_2  26.1 MiB   conda  https://conda.anaconda.org/conda-forge/
cuda-nvcc_linux-64           12.9.86  he0b4e1d_1  26.2 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-nvdisasm                12.9.88  hbd13f7d_0  5.3 MiB    conda  https://conda.anaconda.org/conda-forge/
cuda-nvml-dev                12.9.79  hbd13f7d_0  139.1 KiB  conda  https://conda.anaconda.org/conda-forge/
cuda-nvprof                  12.9.79  hcf8d014_0  2.5 MiB    conda  https://conda.anaconda.org/conda-forge/
cuda-nvprune                 12.9.82  hbd13f7d_0  69.3 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-nvrtc                   12.9.86  h5888daf_0  64.1 MiB   conda  https://conda.anaconda.org/conda-forge/
cuda-nvrtc-dev               12.9.86  h5888daf_0  35.7 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-nvtx                    12.9.79  h5888daf_0  28.6 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-nvvm-dev_linux-64       12.9.86  ha770c72_2  26.5 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-nvvm-impl               12.9.86  h4bc722e_2  20.4 MiB   conda  https://conda.anaconda.org/conda-forge/
cuda-nvvm-tools              12.9.86  h4bc722e_2  23.1 MiB   conda  https://conda.anaconda.org/conda-forge/
cuda-nvvp                    12.9.79  hbd13f7d_0  104.3 MiB  conda  https://conda.anaconda.org/conda-forge/
cuda-opencl                  12.9.19  h5888daf_0  30 KiB     conda  https://conda.anaconda.org/conda-forge/
cuda-opencl-dev              12.9.19  h5888daf_0  95.1 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-profiler-api            12.9.79  h7938cbb_0  23 KiB     conda  https://conda.anaconda.org/conda-forge/
cuda-runtime                 12.9.1   ha804496_0  19.9 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-sanitizer-api           12.9.79  hcf8d014_0  8.6 MiB    conda  https://conda.anaconda.org/conda-forge/
cuda-toolkit                 12.9.1   ha804496_0  20 KiB     conda  https://conda.anaconda.org/conda-forge/
cuda-tools                   12.9.1   ha770c72_0  19.9 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-version                 12.9     h4f385c5_3  21.1 KiB   conda  https://conda.anaconda.org/conda-forge/
cuda-visual-tools            12.9.1   ha770c72_0  19.9 KiB   conda  https://conda.anaconda.org/conda-forge/</code></pre>
</div>
<p>To “prove” that this works, we can ask for the CUDA enabled version
of PyTorch</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="ex">pixi</span> add <span class="at">--platform</span> linux-64 pytorch-gpu</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>✔ Added pytorch-gpu &gt;=2.7.1,&lt;3
Added these only for platform(s): linux-64</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="ex">pixi</span> list <span class="at">--platform</span> linux-64 torch</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Package      Version  Build                           Size       Kind   Source
libtorch     2.7.1    cuda129_mkl_h9562ed8_304        836.3 MiB  conda  https://conda.anaconda.org/conda-forge/
pytorch      2.7.1    cuda129_mkl_py313_h1e53aa0_304  28.1 MiB   conda  https://conda.anaconda.org/conda-forge/
pytorch-gpu  2.7.1    cuda129_mkl_h43a4b0b_304        46.9 KiB   conda  https://conda.anaconda.org/conda-forge/</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">TOML<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode toml" tabindex="0"><code class="sourceCode toml"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a><span class="kw">[workspace]</span></span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a><span class="dt">channels</span> <span class="op">=</span> <span class="op">[</span><span class="st">"conda-forge"</span><span class="op">]</span></span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a><span class="dt">name</span> <span class="op">=</span> <span class="st">"cuda-example"</span></span>
<span id="cb27-4"><a href="#cb27-4" tabindex="-1"></a><span class="dt">platforms</span> <span class="op">=</span> <span class="op">[</span><span class="st">"linux-64"</span><span class="op">,</span> <span class="st">"osx-arm64"</span><span class="op">,</span> <span class="st">"win-64"</span><span class="op">]</span></span>
<span id="cb27-5"><a href="#cb27-5" tabindex="-1"></a><span class="dt">version</span> <span class="op">=</span> <span class="st">"0.1.0"</span></span>
<span id="cb27-6"><a href="#cb27-6" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" tabindex="-1"></a><span class="kw">[tasks]</span></span>
<span id="cb27-8"><a href="#cb27-8" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" tabindex="-1"></a><span class="kw">[dependencies]</span></span>
<span id="cb27-10"><a href="#cb27-10" tabindex="-1"></a><span class="dt">cuda-version</span> <span class="op">=</span> <span class="st">"12.9.*"</span></span>
<span id="cb27-11"><a href="#cb27-11" tabindex="-1"></a></span>
<span id="cb27-12"><a href="#cb27-12" tabindex="-1"></a><span class="kw">[system-requirements]</span></span>
<span id="cb27-13"><a href="#cb27-13" tabindex="-1"></a><span class="dt">cuda</span> <span class="op">=</span> <span class="st">"12"</span></span>
<span id="cb27-14"><a href="#cb27-14" tabindex="-1"></a></span>
<span id="cb27-15"><a href="#cb27-15" tabindex="-1"></a><span class="kw">[target.linux-64.dependencies]</span></span>
<span id="cb27-16"><a href="#cb27-16" tabindex="-1"></a><span class="dt">cuda</span> <span class="op">=</span> <span class="st">"&gt;=12.9.1,&lt;13"</span></span>
<span id="cb27-17"><a href="#cb27-17" tabindex="-1"></a><span class="dt">pytorch-gpu</span> <span class="op">=</span> <span class="st">"&gt;=2.7.1,&lt;3"</span></span></code></pre>
</div>
<div id="redundancy-in-example" class="callout caution">
<div class="callout-square">
<i class="callout-icon" data-feather="alert-triangle"></i>
</div>
<span class="callout-header">Caution</span>
<div id="redundancy-in-example" class="callout-inner">
<h3 class="callout-title">Redundancy in example</h3>
<div class="callout-content">
<p>Note that we added the <code>cuda</code> package here for
demonstraton purposes, but we didn’t <em>need</em> to as it would
already be installed as a dependency of <code>pytorch-gpu</code>.</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a><span class="fu">cat</span> .pixi/envs/default/conda-meta/pytorch-gpu-<span class="pp">*</span>.json</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">JSON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode json" tabindex="0"><code class="sourceCode json"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a>  <span class="dt">"build"</span><span class="fu">:</span> <span class="st">"cuda129_mkl_h43a4b0b_304"</span><span class="fu">,</span></span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a>  <span class="dt">"build_number"</span><span class="fu">:</span> <span class="dv">304</span><span class="fu">,</span></span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a>  <span class="dt">"depends"</span><span class="fu">:</span> <span class="ot">[</span></span>
<span id="cb29-5"><a href="#cb29-5" tabindex="-1"></a>    <span class="st">"pytorch 2.7.1 cuda*_mkl*304"</span></span>
<span id="cb29-6"><a href="#cb29-6" tabindex="-1"></a>  <span class="ot">]</span><span class="fu">,</span></span>
<span id="cb29-7"><a href="#cb29-7" tabindex="-1"></a>  <span class="dt">"license"</span><span class="fu">:</span> <span class="st">"BSD-3-Clause"</span><span class="fu">,</span></span>
<span id="cb29-8"><a href="#cb29-8" tabindex="-1"></a>  <span class="dt">"license_family"</span><span class="fu">:</span> <span class="st">"BSD"</span><span class="fu">,</span></span>
<span id="cb29-9"><a href="#cb29-9" tabindex="-1"></a>  <span class="dt">"md5"</span><span class="fu">:</span> <span class="st">"e374ee50f7d5171d82320bced8165e85"</span><span class="fu">,</span></span>
<span id="cb29-10"><a href="#cb29-10" tabindex="-1"></a>  <span class="dt">"name"</span><span class="fu">:</span> <span class="st">"pytorch-gpu"</span><span class="fu">,</span></span>
<span id="cb29-11"><a href="#cb29-11" tabindex="-1"></a>  <span class="dt">"sha256"</span><span class="fu">:</span> <span class="st">"af54e6535619f4e484d278d015df6ea67622e2194f78da2c0541958fc3d83d18"</span><span class="fu">,</span></span>
<span id="cb29-12"><a href="#cb29-12" tabindex="-1"></a>  <span class="dt">"size"</span><span class="fu">:</span> <span class="dv">48008</span><span class="fu">,</span></span>
<span id="cb29-13"><a href="#cb29-13" tabindex="-1"></a>  <span class="dt">"subdir"</span><span class="fu">:</span> <span class="st">"linux-64"</span><span class="fu">,</span></span>
<span id="cb29-14"><a href="#cb29-14" tabindex="-1"></a>  <span class="dt">"timestamp"</span><span class="fu">:</span> <span class="dv">1753886159800</span><span class="fu">,</span></span>
<span id="cb29-15"><a href="#cb29-15" tabindex="-1"></a>  <span class="dt">"version"</span><span class="fu">:</span> <span class="st">"2.7.1"</span><span class="fu">,</span></span>
<span id="cb29-16"><a href="#cb29-16" tabindex="-1"></a>  <span class="dt">"fn"</span><span class="fu">:</span> <span class="st">"pytorch-gpu-2.7.1-cuda129_mkl_h43a4b0b_304.conda"</span><span class="fu">,</span></span>
<span id="cb29-17"><a href="#cb29-17" tabindex="-1"></a>  <span class="dt">"url"</span><span class="fu">:</span> <span class="st">"https://conda.anaconda.org/conda-forge/linux-64/pytorch-gpu-2.7.1-cuda129_mkl_h43a4b0b_304.conda"</span><span class="fu">,</span></span>
<span id="cb29-18"><a href="#cb29-18" tabindex="-1"></a>  <span class="dt">"channel"</span><span class="fu">:</span> <span class="st">"https://conda.anaconda.org/conda-forge/"</span><span class="fu">,</span></span>
<span id="cb29-19"><a href="#cb29-19" tabindex="-1"></a>  <span class="dt">"extracted_package_dir"</span><span class="fu">:</span> <span class="st">"/home/&lt;username&gt;/.cache/rattler/cache/pkgs/pytorch-gpu-2.7.1-cuda129_mkl_h43a4b0b_304"</span><span class="fu">,</span></span>
<span id="cb29-20"><a href="#cb29-20" tabindex="-1"></a>  <span class="dt">"files"</span><span class="fu">:</span> <span class="ot">[]</span><span class="fu">,</span></span>
<span id="cb29-21"><a href="#cb29-21" tabindex="-1"></a>  <span class="dt">"paths_data"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb29-22"><a href="#cb29-22" tabindex="-1"></a>    <span class="dt">"paths_version"</span><span class="fu">:</span> <span class="dv">1</span><span class="fu">,</span></span>
<span id="cb29-23"><a href="#cb29-23" tabindex="-1"></a>    <span class="dt">"paths"</span><span class="fu">:</span> <span class="ot">[]</span></span>
<span id="cb29-24"><a href="#cb29-24" tabindex="-1"></a>  <span class="fu">},</span></span>
<span id="cb29-25"><a href="#cb29-25" tabindex="-1"></a>  <span class="dt">"link"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb29-26"><a href="#cb29-26" tabindex="-1"></a>    <span class="dt">"source"</span><span class="fu">:</span> <span class="st">"/home/&lt;username&gt;/.cache/rattler/cache/pkgs/pytorch-gpu-2.7.1-cuda129_mkl_h43a4b0b_304"</span><span class="fu">,</span></span>
<span id="cb29-27"><a href="#cb29-27" tabindex="-1"></a>    <span class="dt">"type"</span><span class="fu">:</span> <span class="dv">1</span></span>
<span id="cb29-28"><a href="#cb29-28" tabindex="-1"></a>  <span class="fu">}</span></span>
<span id="cb29-29"><a href="#cb29-29" tabindex="-1"></a><span class="fu">}</span></span></code></pre>
</div>
</div>
</div>
</div>
<p>and <strong>if on the supported <code>linux-64</code> platform with a
valid <code>__cuda</code> virtual pacakge</strong> check that it can see
and find GPUs</p>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a><span class="co"># torch_detect_GPU.py</span></span>
<span id="cb30-2"><a href="#cb30-2" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb30-3"><a href="#cb30-3" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> cuda</span>
<span id="cb30-4"><a href="#cb30-4" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb30-6"><a href="#cb30-6" tabindex="-1"></a>    <span class="cf">if</span> torch.backends.cuda.is_built():</span>
<span id="cb30-7"><a href="#cb30-7" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"PyTorch build CUDA version: </span><span class="sc">{</span>torch<span class="sc">.</span>version<span class="sc">.</span>cuda<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-8"><a href="#cb30-8" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"PyTorch build cuDNN version: </span><span class="sc">{</span>torch<span class="sc">.</span>backends<span class="sc">.</span>cudnn<span class="sc">.</span>version()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-9"><a href="#cb30-9" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"PyTorch build NCCL version: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>nccl<span class="sc">.</span>version()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-10"><a href="#cb30-10" tabindex="-1"></a></span>
<span id="cb30-11"><a href="#cb30-11" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Number of GPUs found on system: </span><span class="sc">{</span>cuda<span class="sc">.</span>device_count()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-12"><a href="#cb30-12" tabindex="-1"></a></span>
<span id="cb30-13"><a href="#cb30-13" tabindex="-1"></a>    <span class="cf">if</span> cuda.is_available():</span>
<span id="cb30-14"><a href="#cb30-14" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Active GPU index: </span><span class="sc">{</span>cuda<span class="sc">.</span>current_device()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-15"><a href="#cb30-15" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Active GPU name: </span><span class="sc">{</span>cuda<span class="sc">.</span>get_device_name(cuda.current_device())<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-16"><a href="#cb30-16" tabindex="-1"></a>    <span class="cf">elif</span> torch.backends.mps.is_available():</span>
<span id="cb30-17"><a href="#cb30-17" tabindex="-1"></a>        mps_device <span class="op">=</span> torch.device(<span class="st">"mps"</span>)</span>
<span id="cb30-18"><a href="#cb30-18" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"PyTorch has active GPU: </span><span class="sc">{</span>mps_device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-19"><a href="#cb30-19" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb30-20"><a href="#cb30-20" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"PyTorch has no active GPU"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a><span class="ex">pixi</span> run python torch_detect_GPU.py</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>PyTorch build CUDA version: 12.9
PyTorch build cuDNN version: 91100
PyTorch build NCCL version: (2, 27, 7)

Number of GPUs found on system: 1

Active GPU index: 0
Active GPU name: NVIDIA GeForce RTX 4060 Laptop GPU</code></pre>
</div>
<div id="multi-environment-pixi-workspaces" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="multi-environment-pixi-workspaces" class="callout-inner">
<h3 class="callout-title">Multi-environment Pixi workspaces</h3>
<div class="callout-content">
<p>Create a new Pixi workspace that:</p>
<ul><li>Contains an environment for <code>linux-64</code>,
<code>osx-arm64</code>, and <code>win-64</code> that supports the CPU
version of PyTorch</li>
<li>Contains an environment for <code>linux-64</code> and
<code>win-64</code> that supports the GPU version of PyTorch</li>
<li>Supports CUDA <code>v12</code>
</li>
</ul></div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>Create a new workspace</p>
<div class="codewrapper sourceCode" id="cb33">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb33-1"><a href="#cb33-1" tabindex="-1"></a><span class="ex">pixi</span> init ~/pixi-cuda-lesson/cuda-exercise</span>
<span id="cb33-2"><a href="#cb33-2" tabindex="-1"></a><span class="bu">cd</span> ~/pixi-cuda-lesson/cuda-exercise</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>✔ Created /home/&lt;username&gt;/pixi-cuda-lesson/cuda-exercise/pixi.toml</code></pre>
</div>
<p>Add support for all the target platforms</p>
<div class="codewrapper sourceCode" id="cb35">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb35-1"><a href="#cb35-1" tabindex="-1"></a><span class="ex">pixi</span> workspace platform add linux-64 osx-arm64 win-64</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>✔ Added linux-64
✔ Added osx-arm64
✔ Added win-64</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb37">
<h3 class="code-label">TOML<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode toml" tabindex="0"><code class="sourceCode toml"><span id="cb37-1"><a href="#cb37-1" tabindex="-1"></a><span class="kw">[workspace]</span></span>
<span id="cb37-2"><a href="#cb37-2" tabindex="-1"></a><span class="dt">channels</span> <span class="op">=</span> <span class="op">[</span><span class="st">"conda-forge"</span><span class="op">]</span></span>
<span id="cb37-3"><a href="#cb37-3" tabindex="-1"></a><span class="dt">name</span> <span class="op">=</span> <span class="st">"cuda-exercise"</span></span>
<span id="cb37-4"><a href="#cb37-4" tabindex="-1"></a><span class="dt">platforms</span> <span class="op">=</span> <span class="op">[</span><span class="st">"linux-64"</span><span class="op">,</span> <span class="st">"osx-arm64"</span><span class="op">,</span> <span class="st">"win-64"</span><span class="op">]</span></span>
<span id="cb37-5"><a href="#cb37-5" tabindex="-1"></a><span class="dt">version</span> <span class="op">=</span> <span class="st">"0.1.0"</span></span>
<span id="cb37-6"><a href="#cb37-6" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" tabindex="-1"></a><span class="kw">[tasks]</span></span>
<span id="cb37-8"><a href="#cb37-8" tabindex="-1"></a></span>
<span id="cb37-9"><a href="#cb37-9" tabindex="-1"></a><span class="kw">[dependencies]</span></span></code></pre>
</div>
<p>Let’s first add <code>pytorch-cpu</code> to a <code>cpu</code>
feature</p>
<div class="codewrapper sourceCode" id="cb38">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb38-1"><a href="#cb38-1" tabindex="-1"></a><span class="ex">pixi</span> add <span class="at">--feature</span> cpu pytorch-cpu</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>✔ Added pytorch-cpu
Added these only for feature: cpu</code></pre>
</div>
<p>and then create a <code>cpu</code> environment that contains the
<code>cpu</code> feature</p>
<div class="codewrapper sourceCode" id="cb40">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb40-1"><a href="#cb40-1" tabindex="-1"></a><span class="ex">pixi</span> workspace environment add <span class="at">--feature</span> cpu cpu</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>✔ Added environment cpu</code></pre>
</div>
<p>and then instantiate the <code>pytorch-cpu</code> package with a
particular version and solve through <a href="https://pixi.sh/dev/reference/cli/pixi/upgrade/" class="external-link"><code>pixi upgrade</code></a>
(or could readd the package to the feature)</p>
<div class="codewrapper sourceCode" id="cb42">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb42-1"><a href="#cb42-1" tabindex="-1"></a><span class="ex">pixi</span> upgrade <span class="at">--feature</span> cpu pytorch-cpu</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb43">
<h3 class="code-label">TOML<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode toml" tabindex="0"><code class="sourceCode toml"><span id="cb43-1"><a href="#cb43-1" tabindex="-1"></a><span class="kw">[workspace]</span></span>
<span id="cb43-2"><a href="#cb43-2" tabindex="-1"></a><span class="dt">channels</span> <span class="op">=</span> <span class="op">[</span><span class="st">"conda-forge"</span><span class="op">]</span></span>
<span id="cb43-3"><a href="#cb43-3" tabindex="-1"></a><span class="dt">name</span> <span class="op">=</span> <span class="st">"cuda-exercise"</span></span>
<span id="cb43-4"><a href="#cb43-4" tabindex="-1"></a><span class="dt">platforms</span> <span class="op">=</span> <span class="op">[</span><span class="st">"linux-64"</span><span class="op">,</span> <span class="st">"osx-arm64"</span><span class="op">,</span> <span class="st">"win-64"</span><span class="op">]</span></span>
<span id="cb43-5"><a href="#cb43-5" tabindex="-1"></a><span class="dt">version</span> <span class="op">=</span> <span class="st">"0.1.0"</span></span>
<span id="cb43-6"><a href="#cb43-6" tabindex="-1"></a></span>
<span id="cb43-7"><a href="#cb43-7" tabindex="-1"></a><span class="kw">[tasks]</span></span>
<span id="cb43-8"><a href="#cb43-8" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" tabindex="-1"></a><span class="kw">[dependencies]</span></span>
<span id="cb43-10"><a href="#cb43-10" tabindex="-1"></a></span>
<span id="cb43-11"><a href="#cb43-11" tabindex="-1"></a><span class="kw">[feature.cpu.dependencies]</span></span>
<span id="cb43-12"><a href="#cb43-12" tabindex="-1"></a><span class="dt">pytorch-cpu</span> <span class="op">=</span> <span class="st">"&gt;=2.8.0,&lt;3"</span></span>
<span id="cb43-13"><a href="#cb43-13" tabindex="-1"></a></span>
<span id="cb43-14"><a href="#cb43-14" tabindex="-1"></a><span class="kw">[environments]</span></span>
<span id="cb43-15"><a href="#cb43-15" tabindex="-1"></a><span class="dt">cpu</span> <span class="op">=</span> <span class="op">[</span><span class="st">"cpu"</span><span class="op">]</span></span></code></pre>
</div>
<p>Now, for the GPU environment, add CUDA system-requirements for
<code>linux-64</code> for the <code>gpu</code> feature</p>
<div class="codewrapper sourceCode" id="cb44">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb44-1"><a href="#cb44-1" tabindex="-1"></a><span class="ex">pixi</span> workspace system-requirements add <span class="at">--feature</span> gpu cuda 12</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb45">
<h3 class="code-label">TOML<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode toml" tabindex="0"><code class="sourceCode toml"><span id="cb45-1"><a href="#cb45-1" tabindex="-1"></a><span class="kw">[workspace]</span></span>
<span id="cb45-2"><a href="#cb45-2" tabindex="-1"></a><span class="dt">channels</span> <span class="op">=</span> <span class="op">[</span><span class="st">"conda-forge"</span><span class="op">]</span></span>
<span id="cb45-3"><a href="#cb45-3" tabindex="-1"></a><span class="dt">name</span> <span class="op">=</span> <span class="st">"cuda-exercise"</span></span>
<span id="cb45-4"><a href="#cb45-4" tabindex="-1"></a><span class="dt">platforms</span> <span class="op">=</span> <span class="op">[</span><span class="st">"linux-64"</span><span class="op">,</span> <span class="st">"osx-arm64"</span><span class="op">,</span> <span class="st">"win-64"</span><span class="op">]</span></span>
<span id="cb45-5"><a href="#cb45-5" tabindex="-1"></a><span class="dt">version</span> <span class="op">=</span> <span class="st">"0.1.0"</span></span>
<span id="cb45-6"><a href="#cb45-6" tabindex="-1"></a></span>
<span id="cb45-7"><a href="#cb45-7" tabindex="-1"></a><span class="kw">[tasks]</span></span>
<span id="cb45-8"><a href="#cb45-8" tabindex="-1"></a></span>
<span id="cb45-9"><a href="#cb45-9" tabindex="-1"></a><span class="kw">[dependencies]</span></span>
<span id="cb45-10"><a href="#cb45-10" tabindex="-1"></a></span>
<span id="cb45-11"><a href="#cb45-11" tabindex="-1"></a><span class="kw">[feature.cpu.dependencies]</span></span>
<span id="cb45-12"><a href="#cb45-12" tabindex="-1"></a><span class="dt">pytorch-cpu</span> <span class="op">=</span> <span class="st">"&gt;=2.8.0,&lt;3"</span></span>
<span id="cb45-13"><a href="#cb45-13" tabindex="-1"></a></span>
<span id="cb45-14"><a href="#cb45-14" tabindex="-1"></a><span class="kw">[feature.gpu.system-requirements]</span></span>
<span id="cb45-15"><a href="#cb45-15" tabindex="-1"></a><span class="dt">cuda</span> <span class="op">=</span> <span class="st">"12"</span></span>
<span id="cb45-16"><a href="#cb45-16" tabindex="-1"></a></span>
<span id="cb45-17"><a href="#cb45-17" tabindex="-1"></a><span class="kw">[environments]</span></span>
<span id="cb45-18"><a href="#cb45-18" tabindex="-1"></a><span class="dt">cpu</span> <span class="op">=</span> <span class="op">[</span><span class="st">"cpu"</span><span class="op">]</span></span></code></pre>
</div>
<p>and create a <code>gpu</code> environment with the <code>gpu</code>
feature</p>
<div class="codewrapper sourceCode" id="cb46">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb46-1"><a href="#cb46-1" tabindex="-1"></a><span class="ex">pixi</span> workspace environment add <span class="at">--feature</span> gpu gpu</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>✔ Added environment gpu</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb48">
<h3 class="code-label">TOML<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode toml" tabindex="0"><code class="sourceCode toml"><span id="cb48-1"><a href="#cb48-1" tabindex="-1"></a><span class="kw">[workspace]</span></span>
<span id="cb48-2"><a href="#cb48-2" tabindex="-1"></a><span class="dt">channels</span> <span class="op">=</span> <span class="op">[</span><span class="st">"conda-forge"</span><span class="op">]</span></span>
<span id="cb48-3"><a href="#cb48-3" tabindex="-1"></a><span class="dt">name</span> <span class="op">=</span> <span class="st">"cuda-exercise"</span></span>
<span id="cb48-4"><a href="#cb48-4" tabindex="-1"></a><span class="dt">platforms</span> <span class="op">=</span> <span class="op">[</span><span class="st">"linux-64"</span><span class="op">,</span> <span class="st">"osx-arm64"</span><span class="op">,</span> <span class="st">"win-64"</span><span class="op">]</span></span>
<span id="cb48-5"><a href="#cb48-5" tabindex="-1"></a><span class="dt">version</span> <span class="op">=</span> <span class="st">"0.1.0"</span></span>
<span id="cb48-6"><a href="#cb48-6" tabindex="-1"></a></span>
<span id="cb48-7"><a href="#cb48-7" tabindex="-1"></a><span class="kw">[tasks]</span></span>
<span id="cb48-8"><a href="#cb48-8" tabindex="-1"></a></span>
<span id="cb48-9"><a href="#cb48-9" tabindex="-1"></a><span class="kw">[dependencies]</span></span>
<span id="cb48-10"><a href="#cb48-10" tabindex="-1"></a></span>
<span id="cb48-11"><a href="#cb48-11" tabindex="-1"></a><span class="kw">[feature.cpu.dependencies]</span></span>
<span id="cb48-12"><a href="#cb48-12" tabindex="-1"></a><span class="dt">pytorch-cpu</span> <span class="op">=</span> <span class="st">"&gt;=2.8.0,&lt;3"</span></span>
<span id="cb48-13"><a href="#cb48-13" tabindex="-1"></a></span>
<span id="cb48-14"><a href="#cb48-14" tabindex="-1"></a><span class="kw">[feature.gpu.system-requirements]</span></span>
<span id="cb48-15"><a href="#cb48-15" tabindex="-1"></a><span class="dt">cuda</span> <span class="op">=</span> <span class="st">"12"</span></span>
<span id="cb48-16"><a href="#cb48-16" tabindex="-1"></a></span>
<span id="cb48-17"><a href="#cb48-17" tabindex="-1"></a><span class="kw">[environments]</span></span>
<span id="cb48-18"><a href="#cb48-18" tabindex="-1"></a><span class="dt">cpu</span> <span class="op">=</span> <span class="op">[</span><span class="st">"cpu"</span><span class="op">]</span></span>
<span id="cb48-19"><a href="#cb48-19" tabindex="-1"></a><span class="dt">gpu</span> <span class="op">=</span> <span class="op">[</span><span class="st">"gpu"</span><span class="op">]</span></span></code></pre>
</div>
<p>then add the <code>pytorch-gpu</code> pacakge for
<code>linux-64</code> and <code>win-64</code> to the <code>gpu</code>
feature</p>
<div class="codewrapper sourceCode" id="cb49">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb49-1"><a href="#cb49-1" tabindex="-1"></a><span class="ex">pixi</span> add <span class="at">--platform</span> linux-64 <span class="at">--platform</span> win-64 <span class="at">--feature</span> gpu pytorch-gpu</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>✔ Added pytorch-gpu &gt;=2.8.0,&lt;3
Added these only for platform(s): linux-64, win-64
Added these only for feature: gpu</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb51">
<h3 class="code-label">TOML<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode toml" tabindex="0"><code class="sourceCode toml"><span id="cb51-1"><a href="#cb51-1" tabindex="-1"></a><span class="kw">[workspace]</span></span>
<span id="cb51-2"><a href="#cb51-2" tabindex="-1"></a><span class="dt">channels</span> <span class="op">=</span> <span class="op">[</span><span class="st">"conda-forge"</span><span class="op">]</span></span>
<span id="cb51-3"><a href="#cb51-3" tabindex="-1"></a><span class="dt">name</span> <span class="op">=</span> <span class="st">"cuda-exercise"</span></span>
<span id="cb51-4"><a href="#cb51-4" tabindex="-1"></a><span class="dt">platforms</span> <span class="op">=</span> <span class="op">[</span><span class="st">"linux-64"</span><span class="op">,</span> <span class="st">"osx-arm64"</span><span class="op">,</span> <span class="st">"win-64"</span><span class="op">]</span></span>
<span id="cb51-5"><a href="#cb51-5" tabindex="-1"></a><span class="dt">version</span> <span class="op">=</span> <span class="st">"0.1.0"</span></span>
<span id="cb51-6"><a href="#cb51-6" tabindex="-1"></a></span>
<span id="cb51-7"><a href="#cb51-7" tabindex="-1"></a><span class="kw">[tasks]</span></span>
<span id="cb51-8"><a href="#cb51-8" tabindex="-1"></a></span>
<span id="cb51-9"><a href="#cb51-9" tabindex="-1"></a><span class="kw">[dependencies]</span></span>
<span id="cb51-10"><a href="#cb51-10" tabindex="-1"></a></span>
<span id="cb51-11"><a href="#cb51-11" tabindex="-1"></a><span class="kw">[feature.cpu.dependencies]</span></span>
<span id="cb51-12"><a href="#cb51-12" tabindex="-1"></a><span class="dt">pytorch-cpu</span> <span class="op">=</span> <span class="st">"&gt;=2.8.0,&lt;3"</span></span>
<span id="cb51-13"><a href="#cb51-13" tabindex="-1"></a></span>
<span id="cb51-14"><a href="#cb51-14" tabindex="-1"></a><span class="kw">[feature.gpu.system-requirements]</span></span>
<span id="cb51-15"><a href="#cb51-15" tabindex="-1"></a><span class="dt">cuda</span> <span class="op">=</span> <span class="st">"12"</span></span>
<span id="cb51-16"><a href="#cb51-16" tabindex="-1"></a></span>
<span id="cb51-17"><a href="#cb51-17" tabindex="-1"></a><span class="kw">[feature.gpu.target.linux-64.dependencies]</span></span>
<span id="cb51-18"><a href="#cb51-18" tabindex="-1"></a><span class="dt">pytorch-gpu</span> <span class="op">=</span> <span class="st">"&gt;=2.8.0,&lt;3"</span></span>
<span id="cb51-19"><a href="#cb51-19" tabindex="-1"></a></span>
<span id="cb51-20"><a href="#cb51-20" tabindex="-1"></a><span class="kw">[feature.gpu.target.win-64.dependencies]</span></span>
<span id="cb51-21"><a href="#cb51-21" tabindex="-1"></a><span class="dt">pytorch-gpu</span> <span class="op">=</span> <span class="st">"&gt;=2.8.0,&lt;3"</span></span>
<span id="cb51-22"><a href="#cb51-22" tabindex="-1"></a></span>
<span id="cb51-23"><a href="#cb51-23" tabindex="-1"></a><span class="kw">[environments]</span></span>
<span id="cb51-24"><a href="#cb51-24" tabindex="-1"></a><span class="dt">cpu</span> <span class="op">=</span> <span class="op">[</span><span class="st">"cpu"</span><span class="op">]</span></span>
<span id="cb51-25"><a href="#cb51-25" tabindex="-1"></a><span class="dt">gpu</span> <span class="op">=</span> <span class="op">[</span><span class="st">"gpu"</span><span class="op">]</span></span></code></pre>
</div>
<p>One can check the environment differences</p>
<div class="codewrapper sourceCode" id="cb52">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb52-1"><a href="#cb52-1" tabindex="-1"></a><span class="ex">pixi</span> list <span class="at">--environment</span> cpu</span>
<span id="cb52-2"><a href="#cb52-2" tabindex="-1"></a><span class="ex">pixi</span> list <span class="at">--environment</span> gpu</span></code></pre>
</div>
<p>and activate shells with different environments loaded</p>
<div class="codewrapper sourceCode" id="cb53">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb53-1"><a href="#cb53-1" tabindex="-1"></a><span class="ex">pixi</span> shell <span class="at">--environment</span> cpu</span></code></pre>
</div>
<p>So in 26 lines of TOML</p>
<div class="codewrapper sourceCode" id="cb54">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb54-1"><a href="#cb54-1" tabindex="-1"></a><span class="fu">wc</span> <span class="at">-l</span> pixi.toml</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>26 pixi.toml</code></pre>
</div>
<p>we created separate CPU and GPU computational environments that are
now fully reproducible with the associated <code>pixi.lock</code>!</p>
</div>
</div>
</div>
</div>
<div id="version-control" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="version-control" class="callout-inner">
<h3 class="callout-title">Version Control</h3>
<div class="callout-content">
<p>On a <strong>new branch</strong> in your repository, add and commit
the files from this episode.</p>
<div class="codewrapper sourceCode" id="cb56">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb56-1"><a href="#cb56-1" tabindex="-1"></a><span class="fu">git</span> add cuda-example/pixi.<span class="pp">*</span> cuda-example/git.<span class="pp">*</span></span>
<span id="cb56-2"><a href="#cb56-2" tabindex="-1"></a><span class="fu">git</span> add cuda-exercise/pixi.<span class="pp">*</span> cuda-exercise/git.<span class="pp">*</span></span></code></pre>
</div>
<p>Then push your branch to your remote on GitHub</p>
<div class="codewrapper sourceCode" id="cb57">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb57-1"><a href="#cb57-1" tabindex="-1"></a><span class="fu">git</span> push <span class="at">-u</span> origin HEAD</span></code></pre>
</div>
<p>and make a pull request to merge your changes into your remote’s
default branch.</p>
</div>
</div>
</div>
<div id="further-cuda-and-gpu-references" class="callout checklist">
<div class="callout-square">
<i class="callout-icon" data-feather="check-square"></i>
</div>
<span class="callout-header">Checklist</span>
<div id="further-cuda-and-gpu-references" class="callout-inner">
<h3 class="callout-title">Further CUDA and GPU references</h3>
<div class="callout-content">
<p>If you would also like a useful summary of different things related
to CUDA, check out Modal’s summary website of CUDA focused GPU
concepts.</p>
<ul><li>
<a href="https://modal.com/gpu-glossary" class="external-link">GPU Glossary</a>, by
Modal</li>
</ul></div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul><li>The <code>cuda-version</code> metapackage can be used to specify
constrains on the versions of the <code>__cuda</code> virtual package
and <code>cudatoolkit</code>.</li>
<li>Pixi can specify a minimum required CUDA version with the
<code>[system-requirements]</code> table.</li>
<li>Pixi can solve environments for platforms that are not the system
platform.</li>
<li>NVIDIA’s open source team and the conda-forge community support the
CUDA conda packages on conda-forge.</li>
<li>The <a href="https://github.com/conda-forge/cuda-feedstock/tree/main/recipe" class="external-link"><code>cuda</code>
metapackage</a> is the primary place to go for user documetnation on the
CUDA conda packages.</li>
</ul></div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/conda-pacakges.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/pixi-deployment.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/conda-pacakges.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Conda packages
        </a>
        <a class="chapter-link float-end" href="../instructor/pixi-deployment.html" rel="next">
          Next: Deploying Pixi...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/carpentries-incubator/reproducible-ml-workflows/edit/main/episodes/cuda-conda-pacakges.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/carpentries-incubator/reproducible-ml-workflows/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries-incubator/reproducible-ml-workflows/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries-incubator/reproducible-ml-workflows/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:matthew.feickert@cern.ch">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.17.1" class="external-link">sandpaper (0.17.1)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.7" class="external-link">varnish (1.0.7)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://carpentries-incubator.github.io/reproducible-ml-workflows/instructor/cuda-conda-pacakges.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, lesson, The Carpentries, machine learning, CUDA, hardware acceleration, reproducible",
  "name": "CUDA conda packages",
  "creativeWorkStatus": "active",
  "url": "https://carpentries-incubator.github.io/reproducible-ml-workflows/instructor/cuda-conda-pacakges.html",
  "identifier": "https://carpentries-incubator.github.io/reproducible-ml-workflows/instructor/cuda-conda-pacakges.html",
  "dateCreated": "2025-03-16",
  "dateModified": "2025-10-28",
  "datePublished": "2025-10-28"
}

  </script><script>
		feather.replace();
	</script></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

